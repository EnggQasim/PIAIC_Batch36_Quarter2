{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ed14597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8a182",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "[Source Kaggle](https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews/data)\n",
    "### classes\n",
    "* 0 - negative\n",
    "* 1 - somewhat negative\n",
    "* 2 - neutral\n",
    "* 3 - somewhat positive\n",
    "* 4 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b052a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "0              1           1   \n",
       "1              2           1   \n",
       "2              3           1   \n",
       "3              4           1   \n",
       "4              5           1   \n",
       "...          ...         ...   \n",
       "156055    156056        8544   \n",
       "156056    156057        8544   \n",
       "156057    156058        8544   \n",
       "156058    156059        8544   \n",
       "156059    156060        8544   \n",
       "\n",
       "                                                   Phrase  Sentiment  \n",
       "0       A series of escapades demonstrating the adage ...          1  \n",
       "1       A series of escapades demonstrating the adage ...          2  \n",
       "2                                                A series          2  \n",
       "3                                                       A          2  \n",
       "4                                                  series          2  \n",
       "...                                                   ...        ...  \n",
       "156055                                          Hearst 's          2  \n",
       "156056                          forced avuncular chortles          1  \n",
       "156057                                 avuncular chortles          3  \n",
       "156058                                          avuncular          2  \n",
       "156059                                           chortles          2  \n",
       "\n",
       "[156060 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/EnggQasim/Saylani-AI-Batch3/main/MachineLearning/NLP/train.tsv\", \n",
    "                 sep=\"\\t\")\n",
    "df.info()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f87c10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6e2d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16278\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    df[df.Sentiment==0], # negative class dataset\n",
    "    df[df.Sentiment==4] # positive class dataset\n",
    "])[[\"Phrase\",\"Sentiment\"]]\n",
    "\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4c8ba54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155946</th>\n",
       "      <td>is laughingly enjoyable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155955</th>\n",
       "      <td>a unique culture that is presented with univer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155961</th>\n",
       "      <td>with universal appeal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156007</th>\n",
       "      <td>really do a great job of anchoring the charact...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156010</th>\n",
       "      <td>a great job of anchoring the characters in the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16278 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Phrase  Sentiment\n",
       "101       would have a hard time sitting through this one          0\n",
       "103             have a hard time sitting through this one          0\n",
       "157     Aggressive self-glorification and a manipulati...          0\n",
       "159       self-glorification and a manipulative whitewash          0\n",
       "201                Trouble Every Day is a plodding mess .          0\n",
       "...                                                   ...        ...\n",
       "155946                            is laughingly enjoyable          1\n",
       "155955  a unique culture that is presented with univer...          1\n",
       "155961                              with universal appeal          1\n",
       "156007  really do a great job of anchoring the charact...          1\n",
       "156010  a great job of anchoring the characters in the...          1\n",
       "\n",
       "[16278 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.Sentiment = df1.Sentiment.map({0:0,\n",
    "                                  4:1})#0=Negative 1=Postive remap values\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4f7b27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Phrase  Sentiment\n",
       "101    would have a hard time sitting through this one          0\n",
       "103          have a hard time sitting through this one          0\n",
       "157  Aggressive self-glorification and a manipulati...          0\n",
       "159    self-glorification and a manipulative whitewash          0\n",
       "201             Trouble Every Day is a plodding mess .          0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1dfc1",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a985b414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mqasim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "import string\n",
    "import timeit\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c086624",
   "metadata": {},
   "source": [
    "### Remove newlines & tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5eac8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is her   first day at this place.  Please,  Be nice to her. '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_newlines_tabs(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n",
    "    Output : This is her first day at this place. Please, Be nice to her. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return Formatted_text\n",
    "\n",
    "remove_newlines_tabs(\"This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37eed6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is her \\ first day at this place.\n",
      " Please,\t Be nice to her.\\n\n"
     ]
    }
   ],
   "source": [
    "print(\"This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb394d",
   "metadata": {},
   "source": [
    "### Strip Html Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e712758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a  nice  place to live. Hello Wolrld  '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_html_tags(text):\n",
    "    \"\"\" \n",
    "    This function will remove all the occurrences of html tags from the text.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of html tags.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is a nice place to live. \n",
    "    Output : This is a nice place to live.  \n",
    "    \"\"\"\n",
    "    # Initiating BeautifulSoup object soup.\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Get all the text other than html tags.\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "strip_html_tags(\" This is a <b>nice</b> place to live.<h1>Hello Wolrld</h1> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d60f216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a <b>nice</b> place to live.<h1>Hello Wolrld</h1>\n"
     ]
    }
   ],
   "source": [
    "print(\"This is a <b>nice</b> place to live.<h1>Hello Wolrld</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac2322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caab93dc",
   "metadata": {},
   "source": [
    "### Remove Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b289a6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' website:   visit:  adasfas ad as'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_links(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of links.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of all types of links.\n",
    "        \n",
    "    Example:\n",
    "    Input : To know more about cats and food & website: catster.com  visit: https://catster.com//how-to-feed-cats\n",
    "\n",
    "    Output : To know more about cats and food & website: visit:     \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing all the occurrences of links that starts with https\n",
    "    remove_https = re.sub(r'http\\S+', '', text)\n",
    "    # Remove all the occurrences of text that ends with .com\n",
    "    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "    return remove_com\n",
    "\n",
    "remove_links(\" website: catster.com  visit: https://catster.com//how-to-feed-cats adasfas ad as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b59a6727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website: catster.com  visit: https://catster.com//how-to-feed-cats adasfas ad as\n"
     ]
    }
   ],
   "source": [
    "print(\"website: catster.com  visit: https://catster.com//how-to-feed-cats adasfas ad as\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0c28a",
   "metadata": {},
   "source": [
    "# Remove WhiteSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9bfc4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you doing ?  (pakistan) ABC hello world!'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\" This function will remove \n",
    "        extra whitespaces from the text\n",
    "    arguments:\n",
    "        input_text: \"text\" of type    \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after extra whitespaces removed .\n",
    "        \n",
    "    Example:\n",
    "    Input : How   are   you   doing   ?\n",
    "    Output : How are you doing ?     \n",
    "        \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    Without_whitespace = re.sub(pattern, ' ', text)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
    "    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    return text    \n",
    "\n",
    "remove_whitespace(\"How   are  \\t you \\n   doing? (pakistan)ABC hello                world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5608e875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How   are  \\t you \\n   doing? (pakistan)ABC'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"How   are  \\t you \\n   doing? (pakistan)ABC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095e640",
   "metadata": {},
   "source": [
    "### Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75282c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Malaga, aeeohello'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for accented characters removal\n",
    "def accented_characters_removal(text):\n",
    "    # this is a docstring\n",
    "    \"\"\"\n",
    "    The function will remove accented characters from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : MÃ¡laga, Ã Ã©ÃªÃ¶hello\n",
    "    Output : Malaga, aeeohello    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "accented_characters_removal(\"MÃ¡laga, Ã Ã©ÃªÃ¶hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528244ad",
   "metadata": {},
   "source": [
    "# Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e5e6881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pakistan zinda bad!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for text lowercasing\n",
    "def lower_casing_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function will convert text into lower case.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "         value: text in lowercase\n",
    "         \n",
    "    Example:\n",
    "    Input : The World is Full of Surprises!\n",
    "    Output : the world is full of surprises!\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert text to lower case\n",
    "    # lower() - It converts all upperase letter of given string to lowercase.\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "lower_casing_text(\"Pakistan Zinda BAD!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac2a39",
   "metadata": {},
   "source": [
    "### Reduce repeated characters and punctuationsÂ¶"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b7654b5",
   "metadata": {},
   "source": [
    "Explanation for using some symbols in above regex expression\n",
    "\\1 â€”> is equivalent to re.search(...). group(1). It Refers to first capturing group. \\1 matches the exact same text that was matched by the first capturing group.\n",
    "\n",
    "{1,} --> It means we are matching for repeatation that occurs more than one times.\n",
    "\n",
    "DOTALL -> It matches newline character as well unlike dot operator which matches everything in the given text except newline character.\n",
    "\n",
    "sub() --> This function is used to replace occurrences of a particular sub-string with another sub-string. This function takes as input the following: The sub-string to replace. The sub-string to replace with.\n",
    "\n",
    "r'\\1\\1' --> It limits all the repeatation to two characters.\n",
    "\n",
    "r'\\1' --> Limits all the repeatation to only one character.\n",
    "\n",
    "{2,} --> It means to match for repeatation that occurs more than two times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32459968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reallyy, Greeaatt !?.;:)'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reducing_incorrect_character_repeatation(text):\n",
    "    \"\"\"\n",
    "    This Function will reduce repeatition to two characters \n",
    "    for alphabets and to one character for punctuations.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Finally formatted text with alphabets repeating to \n",
    "        two characters & punctuations limited to one repeatition \n",
    "        \n",
    "    Example:\n",
    "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
    "    Output : Reallyy, Greeaatt !?.;:)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Pattern matching for all case alphabets\n",
    "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "    \n",
    "    # Limiting all the  repeatation to two characters.\n",
    "    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
    "    \n",
    "    # Pattern matching for all the punctuations that can occur\n",
    "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "    \n",
    "    # Limiting punctuations in previously formatted string to only one.\n",
    "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "    \n",
    "    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "    return Final_Formatted\n",
    "\n",
    "reducing_incorrect_character_repeatation(\"Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1a6a0",
   "metadata": {},
   "source": [
    "# Expand contraction words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7bb6c000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"is not , are not , cannot , cause , cannot've\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "}\n",
    "# The code for expanding contraction words\n",
    "def expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):\n",
    "    \"\"\"expand shortened words to the actual form.\n",
    "       e.g. don't to do not\n",
    "    \n",
    "       arguments:\n",
    "            input_text: \"text\" of type \"String\".\n",
    "         \n",
    "       return:\n",
    "            value: Text with expanded form of shorthened words.\n",
    "        \n",
    "       Example: \n",
    "       Input : ain't, aren't, can't, cause, can't've\n",
    "       Output :  is not, are not, cannot, because, cannot have \n",
    "    \n",
    "     \"\"\"\n",
    "    # Tokenizing text into tokens.\n",
    "    list_Of_tokens = text.split(' ')\n",
    "\n",
    "    # Checking for whether the given token matches with the Key & replacing word with key's value.\n",
    "    \n",
    "    # Check whether Word is in lidt_Of_tokens or not.\n",
    "    for Word in list_Of_tokens: \n",
    "        # Check whether found word is in dictionary \"Contraction Map\" or not as a key. \n",
    "         if Word in CONTRACTION_MAP: \n",
    "                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n",
    "                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n",
    "                \n",
    "    # Converting list of tokens to String.\n",
    "    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
    "    return String_Of_tokens    \n",
    "\n",
    "expand_contractions(\"ain't , aren't , can't , cause , can't've\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94254bc",
   "metadata": {},
   "source": [
    "# Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0e05cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello, K a j a l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code for removing special characters\n",
    "def removing_special_characters(text):\n",
    "    \"\"\"Removing all the special characters except the one that is passed within \n",
    "       the regex to match, as they have imp meaning in the text provided.\n",
    "   \n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text with removed special characters that don't require.\n",
    "        \n",
    "    Example: \n",
    "    Input : Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n",
    "    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n",
    "    \n",
    "   \"\"\"\n",
    "    # The formatted text after removing not necessary punctuations.\n",
    "    Formatted_Text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n",
    "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
    "    return Formatted_Text\n",
    "\n",
    "removing_special_characters(\" Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab2616fe",
   "metadata": {},
   "source": [
    "Punctuations that I am considering Important as per my Dataset.\n",
    ",.?! --> These are some frequent punctuations that occurs a lot and needed to preserved as to understand the context of text.\n",
    "\n",
    ": --> This one is also frequent as per the Dataset. It is important to keep bcz it is giving sense whenever there is a occurrence of time like: 9:05 p.m.\n",
    "\n",
    "% --> This one is also frequently used in many articles and telling more precisely about the data, facts & figures.\n",
    "\n",
    "$ --> This one is used in many articles where prices are considered. So, omitting this symbol will not give much sense about those prices that left as just some numbers only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde201d",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a5aa6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mqasim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f154d414",
   "metadata": {},
   "source": [
    "stopwords.words('chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dadbe176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'This Kajal delhi came study . '\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code for removing stopwords\n",
    "stoplist = stopwords.words('english') \n",
    "\n",
    "stoplist = set(stoplist)\n",
    "def removing_stopwords(text):\n",
    "    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n",
    "       & they can be remove safely without comprimising meaning of the sentence.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after omitted all stopwords.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Kajal from delhi who came here to study.\n",
    "    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n",
    "    \n",
    "   \"\"\"\n",
    "    # repr() function actually gives the precise information about the string\n",
    "    text = repr(text)\n",
    "    # Text without stopwords\n",
    "    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist ]\n",
    "    # Convert list of tokens_without_stopwords to String type.\n",
    "    words_string = ' '.join(No_StopWords)    \n",
    "    return words_string\n",
    "\n",
    "removing_stopwords(\"This is Kajal from the delhi who was came here to study.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af91ee",
   "metadata": {},
   "source": [
    "# Correct mis-spelled words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c50bfe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is Oberoi from Delhi who came hired to study.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code for spelling corrections\n",
    "def spelling_correction(text):\n",
    "    ''' \n",
    "    This function will correct spellings.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after corrected spellings.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Oberois from Dlhi who came heree to studdy.\n",
    "    Output : This is Oberoi from Delhi who came here to study.\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    # Check for spellings in English language\n",
    "    spell = Speller(lang='en')\n",
    "    Corrected_text = spell(text)\n",
    "    return Corrected_text\n",
    "\n",
    "spelling_correction(\"This is Oberois from Dlhi who came hiree to stuydd.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34703f",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f4e5d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mqasim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mqasim/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dfe8b328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'ext',\n",
       " 'have',\n",
       " 'root',\n",
       " 'word',\n",
       " 'only,',\n",
       " 'no',\n",
       " 'tense',\n",
       " 'form,',\n",
       " 'no',\n",
       " 'plural',\n",
       " 'be',\n",
       " 'form',\n",
       " 'cat',\n",
       " 'dog']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code for lemmatization\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    \"\"\"This function converts word to their root words \n",
    "       without explicitely cut down as done in stemming.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : text reduced \n",
    "    Output :  text reduce\n",
    "    \n",
    "   \"\"\"\n",
    "    # Converting words to their root forms\n",
    "    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
    "    return lemma\n",
    "\n",
    "lemmatization(\"cats ext having root words only, no tense form, no plural are forms cats dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cab126a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'education'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('educations','n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a27ca",
   "metadata": {},
   "source": [
    "# Putting all in single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "464436cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\", 'would', 'hard', 'time', 'sit', 'one', 'dog', \"'\"]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Writing main function to merge all the preprocessing steps.\n",
    "def text_preprocessing(text, accented_chars=True, contractions=True, lemmatization_allow = True,\n",
    "                        extra_whitespace=True, newlines_tabs=True, repeatition=True, \n",
    "                       lowercase=True, punctuations=True, mis_spell=True,\n",
    "                       remove_html=True, links=True,  special_chars=True,\n",
    "                       stop_words=True):\n",
    "    \"\"\"\n",
    "    This function will preprocess input text and return\n",
    "    the clean text.\n",
    "    \"\"\"\n",
    "    \n",
    "    if newlines_tabs == True: #remove newlines & tabs.\n",
    "        Data = remove_newlines_tabs(text)\n",
    "        \n",
    "    if remove_html == True: #remove html tags\n",
    "        Data = strip_html_tags(Data)\n",
    "        \n",
    "    if links == True: #remove links\n",
    "        Data = remove_links(Data)\n",
    "        \n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        Data = remove_whitespace(Data)\n",
    "        \n",
    "    if accented_chars == True: #remove accented characters\n",
    "        Data = accented_characters_removal(Data)\n",
    "        \n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        Data = lower_casing_text(Data)\n",
    "        \n",
    "    if repeatition == True: #Reduce repeatitions   \n",
    "        Data = reducing_incorrect_character_repeatation(Data)\n",
    "        \n",
    "    if contractions == True: #expand contractions\n",
    "        Data = expand_contractions(Data)\n",
    "    \n",
    "    if punctuations == True: #remove punctuations\n",
    "        Data = removing_special_characters(Data)\n",
    "    \n",
    "    stoplist = stopwords.words('english') \n",
    "    stoplist = set(stoplist)\n",
    "    \n",
    "    if stop_words == True: #Remove stopwords\n",
    "        Data = removing_stopwords(Data)\n",
    "        \n",
    "    spell = Speller(lang='en')\n",
    "    \n",
    "    if mis_spell == True: #Check for mis-spelled words & correct them.\n",
    "        Data = spelling_correction(Data)\n",
    "        \n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#     print(Data)\n",
    "    if lemmatization_allow == True: #Converts words to lemma form.\n",
    "        Data = lemmatization(Data)\n",
    "    \n",
    "           \n",
    "    return Data\n",
    "text_preprocessing(' would have a hard time sitting through this one dogs ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051ff88",
   "metadata": {},
   "source": [
    "# Now apply function on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5ce8e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Phrase  Sentiment\n",
       "101    would have a hard time sitting through this one          0\n",
       "103          have a hard time sitting through this one          0\n",
       "157  Aggressive self-glorification and a manipulati...          0\n",
       "159    self-glorification and a manipulative whitewash          0\n",
       "201             Trouble Every Day is a plodding mess .          0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f325c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mqasim/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 54s, sys: 32.4 s, total: 40min 26s\n",
      "Wall time: 40min 37s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>input_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['would, hard, time, sit, one, ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['have, hard, time, sit, one, ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "      <td>['aggressive, self, clarification, manipulativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "      <td>['self, clarification, manipulative, whitewash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "      <td>['trouble, every, day, pad, mess, ., ']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Phrase  Sentiment  \\\n",
       "101    would have a hard time sitting through this one          0   \n",
       "103          have a hard time sitting through this one          0   \n",
       "157  Aggressive self-glorification and a manipulati...          0   \n",
       "159    self-glorification and a manipulative whitewash          0   \n",
       "201             Trouble Every Day is a plodding mess .          0   \n",
       "\n",
       "                                            input_data  \n",
       "101                  ['would, hard, time, sit, one, ']  \n",
       "103                   ['have, hard, time, sit, one, ']  \n",
       "157  ['aggressive, self, clarification, manipulativ...  \n",
       "159  ['self, clarification, manipulative, whitewash...  \n",
       "201            ['trouble, every, day, pad, mess, ., ']  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df1['input_data'] = df1.Phrase.apply(text_preprocessing)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9cf1e028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>input_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['would, hard, time, sit, one, ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['have, hard, time, sit, one, ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "      <td>['aggressive, self, clarification, manipulativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "      <td>['self, clarification, manipulative, whitewash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "      <td>['trouble, every, day, pad, mess, ., ']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Phrase  Sentiment  \\\n",
       "101    would have a hard time sitting through this one          0   \n",
       "103          have a hard time sitting through this one          0   \n",
       "157  Aggressive self-glorification and a manipulati...          0   \n",
       "159    self-glorification and a manipulative whitewash          0   \n",
       "201             Trouble Every Day is a plodding mess .          0   \n",
       "\n",
       "                                            input_data  \n",
       "101                  ['would, hard, time, sit, one, ']  \n",
       "103                   ['have, hard, time, sit, one, ']  \n",
       "157  ['aggressive, self, clarification, manipulativ...  \n",
       "159  ['self, clarification, manipulative, whitewash...  \n",
       "201            ['trouble, every, day, pad, mess, ., ']  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.Phrase = df1.Phrase.apply(str)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "974911ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reset_index(drop=True).to_feather(\"data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f263ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d9ab35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>input_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['would, hard, time, sit, one, ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['have, hard, time, sit, one, ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "      <td>['aggressive, self, clarification, manipulativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "      <td>['self, clarification, manipulative, whitewash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "      <td>['trouble, every, day, pad, mess, ., ']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Phrase  Sentiment  \\\n",
       "101    would have a hard time sitting through this one          0   \n",
       "103          have a hard time sitting through this one          0   \n",
       "157  Aggressive self-glorification and a manipulati...          0   \n",
       "159    self-glorification and a manipulative whitewash          0   \n",
       "201             Trouble Every Day is a plodding mess .          0   \n",
       "\n",
       "                                            input_data  \n",
       "101                  ['would, hard, time, sit, one, ']  \n",
       "103                   ['have, hard, time, sit, one, ']  \n",
       "157  ['aggressive, self, clarification, manipulativ...  \n",
       "159  ['self, clarification, manipulative, whitewash...  \n",
       "201            ['trouble, every, day, pad, mess, ., ']  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_pickle(\"data.pkl\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571282a8",
   "metadata": {},
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/#:~:text=To%20convert%20the%20text%20data,text%20data%20to%20numerical%20vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf6a60",
   "metadata": {},
   "source": [
    "# https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0013ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_,\"Vocab\")\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape,\"Shape\")\n",
    "print(type(vector),\"Type\")\n",
    "print(vector.toarray(),\"Array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23308396",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [\"the fox jumped\"]\n",
    "vectorizer.transform(text1).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaad809",
   "metadata": {},
   "source": [
    " ## TF-IDF \n",
    "* Term Frequency: This summarizes how often a given word appears within a document.\n",
    "* Inverse Document Frequency: This downscales words that appear a lot across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfa374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    " \"The dog.\",\n",
    " \"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "# n-gram\n",
    "# TF-IDF\n",
    "# Hashing with HashingVectorizer\n",
    "\n",
    "\n",
    "# WordEmbeding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
